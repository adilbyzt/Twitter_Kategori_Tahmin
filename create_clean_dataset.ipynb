{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mLb1HM4RuMR",
        "outputId": "0bb30383-da7c-4ade-fb24-01317f6f1f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: zeyrek in /usr/local/lib/python3.8/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk>=3 in /usr/local/lib/python3.8/dist-packages (from zeyrek) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3->zeyrek) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3->zeyrek) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3->zeyrek) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3->zeyrek) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install zeyrek"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import zeyrek\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "K89pto1bR56A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AogJIhLJSF0S",
        "outputId": "4b12b74f-a780-4a2c-837b-4b3595a2bd0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"all_tweets.csv\")"
      ],
      "metadata": {
        "id": "Foc35ltHR7_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Belirlenmiş olan kategoriler tanımlanıyor\n",
        "\n",
        "category_0 = \"spor\"\n",
        "category_1 = \"ekonomi\"\n",
        "category_2 = \"siyaset\"\n",
        "category_3 = \"teknoloji & bilim\"\n",
        "\n",
        "\n",
        "#Datasetteki tweetler kategorilere göre ayrılıyor\n",
        "\n",
        "tweets_0 = df[df.Category == category_0] #spor\n",
        "tweets_1 = df[df.Category == category_1] #ekonomi\n",
        "tweets_2 = df[df.Category == category_2] #siyaset\n",
        "tweets_3 = df[df.Category == category_3] #teknoloji & bilim\n",
        "\n",
        "#Datasetinde sadece tweetlerin bulunduğu kolon alınıyor\n",
        "\n",
        "tweets_0 = tweets_0.Tweet.to_numpy()\n",
        "tweets_1 = tweets_1.Tweet.to_numpy()\n",
        "tweets_2 = tweets_2.Tweet.to_numpy()\n",
        "tweets_3 = tweets_3.Tweet.to_numpy()\n",
        "\n",
        "tweets = [tweets_0, tweets_1, tweets_2, tweets_3]\n",
        "tweets_category = [category_0, category_1, category_2, category_3]\n",
        "\n"
      ],
      "metadata": {
        "id": "sQcUgi8KVcYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = zeyrek.MorphAnalyzer() \n",
        "stop_words = set(stopwords.words('turkish')) #stopwordslerin temizlenmesi için stopwordsleri alıyoruz"
      ],
      "metadata": {
        "id": "Tl1gAjVSSKOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#twitterdan çekilen verilerin içerisindeki emojileri silmek için gerekli olan emoji kodları\n",
        "\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)"
      ],
      "metadata": {
        "id": "6CT-T-glSCGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet=[]\n",
        "labels=[]\n",
        "nsw=[]"
      ],
      "metadata": {
        "id": "c-qZBRI5T6Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tweetlerin içerisindeki breakline, noktalama işaretleri, emojiler,stopwordsler gibi istenmeyen veriler çıkarılıyor ve lemmatize işlemi yapılıyor.\n",
        "\n",
        "for i, tweets_set in enumerate(tweets):\n",
        "    for tweet_string in tqdm(tweets_set):  \n",
        "\n",
        "          tweet_string = tweet_string.replace('\\n','') #Line breaks lerden temizliyoruz\n",
        "          tweet_string = re.sub('http://\\S+|https://\\S+', '', tweet_string) #tweetler içerisindeki linkler temizleniyor\n",
        "          tweet_string = tweet_string.translate(str.maketrans(\"\", \"\", string.punctuation)) #tweetlerdeki noktalama işaretleri temizleniyor\n",
        "          tweet_string = emoji_pattern.sub(r'', tweet_string)  #tweet içerisindeki emojiler temizleniyor\n",
        "          lemm_tweet = analyzer.lemmatize(tweet_string)  #tweetler için lemmatize işlemi gerçekleştiriliyor\n",
        "          lemm_tweet=[el[1][-1] for el in lemm_tweet]\n",
        "          for r in lemm_tweet:     \n",
        "              if not r in stop_words: \n",
        "                  nsw.append(r)       #stopwords olmayanlar ayrılıyor ve aşağıda birleştiriliyor\n",
        "                  \n",
        "          clear_tweet =' '.join(nsw)  #dizi string hale getiriliyor\n",
        "          nsw=[]\n",
        "          clear_tweet = clear_tweet.lower() #bütün tweetler küçük harflere çevriliyor\n",
        "          tweet.append(clear_tweet) #temizlenmiş tweetin kendisi listeye ekleniyor\n",
        "     "
      ],
      "metadata": {
        "id": "2Gc-jvqtpwjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Clean\"]=tweet                 #temizlenmiş veriler dataframeye clean kolonu altına kaydediliyor\n",
        "df.to_csv('clean_all_tweets.csv') #dataframe csv dosyası olarak kaydediliyor"
      ],
      "metadata": {
        "id": "KF0cGunMkxHT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}